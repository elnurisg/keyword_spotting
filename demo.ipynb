{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000 # Hz\n",
    "FRAME_LENGTH_MS = 25 # ms\n",
    "FRAME_HOP_MS = 10 # 10 ms\n",
    "FRAMES_BEFORE = 23 \n",
    "FRAMES_AFTER = 8\n",
    "N_MELS = 40\n",
    "frame_length_samples = int(SAMPLE_RATE * FRAME_LENGTH_MS / 1000)\n",
    "frame_hop_samples = int(SAMPLE_RATE * FRAME_HOP_MS / 1000)\n",
    "\n",
    "keywords = [\"_silence_\", \"_unknown_\", \"down\", \"go\", \"left\", \"no\", \"off\", \"on\", \"right\", \"stop\", \"up\", \"yes\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mfcc(audio_file):\n",
    "    \"\"\"Return shape: [n_mfcc, time]\"\"\"\n",
    "    \n",
    "    audio_file, index = librosa.effects.trim(audio_file,\n",
    "                                             hop_length=frame_hop_samples,\n",
    "                                             frame_length=frame_length_samples)\n",
    "\n",
    "    if not isinstance(audio_file, np.ndarray):\n",
    "        audio_file = audio_file.numpy()\n",
    "    y = librosa.feature.melspectrogram(y=audio_file, sr=SAMPLE_RATE, \n",
    "                                        hop_length=frame_hop_samples,\n",
    "                                        n_fft=frame_length_samples,\n",
    "                                        n_mels=128)\n",
    "\n",
    "    # Convert to log scale (dB). We'll use the peak power (max) as reference.\n",
    "    log_S = librosa.power_to_db(y)\n",
    "    log_S_normalized = librosa.util.normalize(log_S)\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(\n",
    "        S=log_S_normalized,\n",
    "        # y=audio_file,\n",
    "        sr=SAMPLE_RATE,\n",
    "        n_mfcc=N_MELS,\n",
    "        hop_length=frame_hop_samples,\n",
    "        n_fft=frame_length_samples\n",
    "    )\n",
    "    \n",
    "    delta_mfcc = librosa.feature.delta(mfccs)\n",
    "\n",
    "    return delta_mfcc\n",
    "\n",
    "\n",
    "def normalize_input_size(data, center_crop=False, target_size=101):\n",
    "# this function is designed for a batch of mfccs which should be 3D\n",
    "    if len(data.shape) == 3:\n",
    "        batch_size, n_mels, N = data.shape\n",
    "\n",
    "        if N == target_size:\n",
    "            return data\n",
    "\n",
    "        if N < target_size:\n",
    "            tot_pads = target_size - N\n",
    "            left_pads = int(np.ceil(tot_pads / 2))\n",
    "            right_pads = int(np.floor(tot_pads / 2))\n",
    "            return np.pad(data, [(0, 0), (0, 0), (left_pads, right_pads)], mode='constant')\n",
    "\n",
    "        if center_crop:\n",
    "            from_ = int((N / 2) - (target_size / 2))\n",
    "        else:\n",
    "            from_ = np.random.randint(0, np.floor(N - target_size))\n",
    "        to_ = from_ + target_size\n",
    "\n",
    "        return data[:, :, from_:to_]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported shape: {}\".format(data.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = tf.keras.models.load_model('CnnModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplerate = 16000  # Hertz\n",
    "duration = 1  # seconds\n",
    "filename = 'output.wav'\n",
    "\n",
    "mydata = sd.rec(int(samplerate * duration), samplerate=samplerate,\n",
    "                channels=1, blocking=True)\n",
    "sf.write(filename, mydata, samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stop'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio, sr = librosa.load(filename, sr=SAMPLE_RATE)\n",
    "input = np.expand_dims(normalize_input_size(compute_mfcc(np.expand_dims(audio, 0))).transpose(0,2,1), -1)\n",
    "output = model_loaded(input)\n",
    "keywords[tf.argmax(output, axis=-1).numpy()[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
